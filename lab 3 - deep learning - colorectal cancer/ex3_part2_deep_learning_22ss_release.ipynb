{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B-kA3MJwu4w"
   },
   "source": [
    "# Lab 3b: Colorectal Cancer Histology\n",
    "*Version 3.0b, Summer Semester 2022*\n",
    "\n",
    "Based on the learnings of the previous lab and the hands-on examples from the [online-videos](https://www.youtube.com/playlist?list=PL1lqZksc3XHQLC8Yb20IG6bXUQkQStd5n), you will now work on a second convolutional neural network. While the first example was great to see the power of deep learning based on images, we want to apply it to a scenario more directly related to healthcare.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "We'll use the [Colorectal Cancer Histology](https://zenodo.org/record/53169#.XGZemKwzbmG) dataset. It was the basis of an article published to *Nature* in 2016 and is [available for free through Open Access](https://www.nature.com/articles/srep27988). Kather et al. achieved an accuracy of 87.4% for a multiclass scenario. Let's see how far you can get. The dataset was then published under Creative Commons license and very recently added to the [TensorFlow example database](https://www.tensorflow.org/datasets/catalog/colorectal_histology), making it easier for us to load and process the data.\n",
    "\n",
    "It includes 5,000 RGB histological images, each 150x150px. These have been classified for 8 different targets (labeled as: 'tumor', 'stroma', 'complex', 'lympho', 'debris', 'mucosa', 'adipose', 'empty'). The following image contains representative images of these classes:\n",
    "\n",
    "![Representative images](https://github.com/andijakl/MachineLearning/raw/main/lab%203%20-%20deep%20learning%20-%20colorectal%20cancer/lab3b-representative-images.jpg) *(a) tumour epithelium, (b) simple stroma, (c) complex stroma (stroma that contains single tumour cells and/or single immune cells), (d) immune cell conglomerates, (e) debris and mucus, (f) mucosal glands, (g) adipose tissue, (h) background.*\n",
    "\n",
    "The dataset is around 260 MB, which still makes it possible to use a standard laptop without dedicated hardware acceleration for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "azH_8sWQwu46",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dec80b86eed257bc149d105bffbd44f",
     "grade": false,
     "grade_id": "cell-2d53132d718ad8e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Your Name\n",
    "\n",
    "Replace the `raise NotImplementedError` with the code `myname = \"\"` and assign your name to the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "KqNdh-Bdwu47",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc4e978fc846dc69988dd013f39fc10",
     "grade": false,
     "grade_id": "name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "F4Bo3jo4wu49",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d10d085043fd4e5c7e1c215257a9c77d",
     "grade": true,
     "grade_id": "name-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert myname != \"\", \"myname should not be empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yab54Lkdwu4_",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e078bf3eebadc02c28383cb04451a3b",
     "grade": false,
     "grade_id": "cell-6bc7429fcfa04b57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1: Initializing\n",
    "\n",
    "First, import `numpy` as `np`, `tensorflow` with the alias `tf`. Next, import `pandas` as `pd`; `matplotlib.pyplot` as `plt`. Configure matplotlib to draw images inline.\n",
    "\n",
    "* If running in the FHSTP JupyterHub or Google Colab environment, all required packages are installed.\n",
    "* If running a local Anaconda installation, please install the `tensorflow` module (e.g., through the \"Environments\" UI in Anaconda Navigator).\n",
    "* If running in Amazon SageMaker, choose the `Python 3 (TensorFlow 2.x Pyton 3.x CPU Optimized)` Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKf-SGIEwu5A"
   },
   "outputs": [],
   "source": [
    "# Note: if running on Amazon SageMaker and matplotlib is not installed, uncomment and run the following line:\n",
    "# !pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "WpLov45pwu5B",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3850e88abb23a449f250de09846fa32f",
     "grade": false,
     "grade_id": "imports",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Place all neccessary imports here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rbmfSePhwu5E",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b70101a90f385a5421f3b98368f8cc3",
     "grade": true,
     "grade_id": "imports-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "from packaging import version\n",
    "assert version.parse(tf.version.VERSION) >= version.parse('2.3.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VoywbKlwu5F"
   },
   "source": [
    "In many real-life cases, you will have a lot of data or an otherwise dynamic process to feed data into your neural network. It doesn't just work to simply load all training examples into memory at once.\n",
    "\n",
    "TensorFlow contains a module called `tensorflow_datasets` ([tfds](https://www.tensorflow.org/datasets/api_docs/python/tfds)), which can automatically download data and provide it in a suitable format for further processing by TensorFlow. Execute the following code block to import the module:\n",
    "\n",
    "* If running in the FHSTP JupyterHub or Google Colab environment, all required packages are installed.\n",
    "* If running a local Anaconda installation, the conda package manager serves `tensorflow_datasets` through the Anaconda Navigator UI, but most likely a very old version (1.2.0). In this case, also install the latest version (4.0.0 or higher) through the pip install code blow.\n",
    "* If running in Amazon SageMaker, uncomment and run the pip install code below. Note: currently, the SageMaker environment seems to have issues executing this notebook due to wrong versions / missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dovBtkjAwu5H"
   },
   "outputs": [],
   "source": [
    "# To install or update the tfds version, uncomment and execute the following code line \n",
    "# in case you don't have version 4.0.0 or higher installed:\n",
    "#!pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RVPHe7-nwu5J",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08b146f09abbb0e358784146eb870aa1",
     "grade": true,
     "grade_id": "tfds-version",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined code: simply execute this cell\n",
    "import tensorflow_datasets as tfds\n",
    "assert version.parse(tfds.__version__) >= version.parse('4.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N08W9LyAwu5K"
   },
   "source": [
    "Next, check which datasets can be loaded through this method. Use the `list_builders()` function of `tfds` (there is also an [online version of this list](https://www.tensorflow.org/datasets/catalog/overview))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "EkW6QQy4wu5K",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bd379baaa2b380d0d22ba51e2b7bcbb",
     "grade": false,
     "grade_id": "tfds-datasets",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "cd842fc3-8a75-477d-82bb-c5bbe9ee5263"
   },
   "outputs": [],
   "source": [
    "# List all available datasets from the tensorflow datasets module\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ft-ozHfwu5M"
   },
   "source": [
    "The example we're looking for is called `colorectal_histology`. The fact that it's contained in `tfds` means that we do not need to worry about downloading the data manually. It also takes care of loading all the individual image files into our input pipeline.\n",
    "\n",
    "*Note:* the `colorectal_histology_large` dataset is different – it contains 10 high-res 5000x5000px images, each containing more than one tissue types. This would then not just require a classification of the whole tissue sample as we're doing, but additionally a localization of where to find the specific classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ew3IMY-2wu5M",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ea11ec92ae08a2b227606b381153e7c",
     "grade": false,
     "grade_id": "cell-a1be4d78a0f03df5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2: Load & Analyze the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4TL5nSQwu5N"
   },
   "source": [
    "Now, it's time to load the dataset. In this dataset, all 5,000 examples come in one piece – there is no pre-defined train / test / validation split. Therefore, we need to define the split manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgzGsADMwu5N"
   },
   "source": [
    "Use `tfds` to `load` the dataset. Use four parameters:\n",
    "\n",
    "* First, the dataset name as a string – simply provide the name of the dataset we want to load. Copy and paste the name of our histology dataset from the available datasets we printed before.\n",
    "* Second, the `split` to use. This dataset provides all samples in a `train` variable. Based on this, we want to use create two sub-dataset variants: 80% for training (`ds_train`), 20% for testing (`ds_test`). Specify this split using `['train[:80%]', 'train[80%:]']` that you assign to the `split` parameter.\n",
    "* Third, set `as_supervised` to `True`. This automatically creates a dataset that contains the input data and the labels as a python tuple. This means that both are contained in a single returned item.\n",
    "* To additionally take a look at further information about the dataset, also set `with_info` to `True`\n",
    "\n",
    "The `tfds.load` function returns two data structures. To assign two structures from return values to variables, simply separate them with a comma:\n",
    "\n",
    "1. A **tuple** that contains **both datasets**, according to the split. Name them `ds_train` and `ds_test` accordingly. A Python tuple is written like this: `(ds_train, ds_test)`\n",
    "2. The **info about the dataset** which we requested to load. Store this in a variable `ds_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "m9d6JMz5wu5O",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ded77714b02476937047240521506e1",
     "grade": false,
     "grade_id": "load-dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the colorectal_histology dataset into the variables: (ds_train, ds_test), ds_info\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6alqd1qTwu5P",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0791dec88f76b2cee98ad2ca46aa2300",
     "grade": true,
     "grade_id": "load_dataset_tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert ds_train.element_spec[0].shape == [150, 150, 3]\n",
    "assert ds_test.element_spec[0].shape == [150, 150, 3]\n",
    "assert isinstance(ds_train, tf.data.Dataset)\n",
    "assert type(ds_info) == tfds.core.dataset_info.DatasetInfo\n",
    "assert ds_info.name == 'colorectal_histology'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqkUOA0nwu5R"
   },
   "source": [
    "Next, print the contents of the `ds_info` variable to get some basic info about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "z06zsyA3wu5S",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e73fe632d4dc8fb28a7fe489a47921b",
     "grade": false,
     "grade_id": "datset-info",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "fe8d9b0c-3385-4c46-ab8f-50c3670f098a"
   },
   "outputs": [],
   "source": [
    "# Print the dataset info\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTAH6T1-wu5T"
   },
   "source": [
    "The `featues[\"label\"]` of `ds_info` contains a bit more information about the target labels we want to classify for. First, assign its `num_classes` property to a variable called `class_count` and also print this new variable you just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "NNDrAM85wu5T",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4d49be7c7485303d48c825bdcc98977",
     "grade": false,
     "grade_id": "num_classes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "9294502a-da85-4f96-acca-fa1c5341cfae"
   },
   "outputs": [],
   "source": [
    "# Assign num_classes contained in ds_info to a variable class_count and print it\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Cxs01I_Bwu5U",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffae74fc8558e9e195d77e5a18ffd788",
     "grade": true,
     "grade_id": "num_classes_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert class_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsIfsADywu5U"
   },
   "source": [
    "You can also retrieve the short target names for each class. To get more information about these, refer to the [original paper](https://www.nature.com/articles/srep27988). Assign the property `names` of `features[\"label\"]` from your dataset info (`ds_info`) to a new variable `class_names` and also print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "xtLUE3Vfwu5U",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3a610a1d8216853f6ba420a43493880",
     "grade": false,
     "grade_id": "class_names",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "cddf174f-796e-49bf-89f4-5aa0aa0abb9f"
   },
   "outputs": [],
   "source": [
    "# Assign the class names to a variable class_names and print it\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Bs20jTJGwu5V",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24a8110d358c3d862046e4dff5c5481d",
     "grade": true,
     "grade_id": "class_names_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert class_names[0] == 'tumor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TOq1v3JZwu5V",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fb3c735a361459fca47ff32116cb72b",
     "grade": false,
     "grade_id": "cell-71538a9311ad73cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3: Take a look at examples\n",
    "\n",
    "Now that we loaded and split the dataset, let's take a look at an instance.\n",
    "\n",
    "In our previous examples, we usually had all data in memory at once (for example, as a Numpy array). Especially for larger image datasets that can easily reach many gigabytes, this is often not possible – the memory requirements would just be too high. Our `ds_train` variable is actually an instance of the [DataSet class](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), which can represent potentially large datasets and provides ways to dynamically load just the parts of the dataset into memory that are currently needed.\n",
    "\n",
    "A simple way to get one example out of the dataset is to use the function `take(1)` from `ds_train`. Store this in a new variable, for example `ds_train_example`. This creates a representation of the dataset with just the first element loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "tO6u3MX8wu5V",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f50fe19b3fe6086502cf017115c56ae",
     "grade": false,
     "grade_id": "take_entry",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Store the first data item into a variable ds_train_example\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J-igR0tHwu5V",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "450a85453b8c6b32377cce635f027a77",
     "grade": true,
     "grade_id": "take_entry_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert len(ds_train_example) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDgN_20iwu5W"
   },
   "source": [
    "Next, just try to print the `ds_train_example` variable to see what types it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "V0CSCBbDwu5W",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5eb84d2179a482236a6dd73641348aee",
     "grade": false,
     "grade_id": "take_entry_print",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "4e38b547-f25c-4d1d-b42d-2efdb7d96089"
   },
   "outputs": [],
   "source": [
    "# Print the dataset example\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-UC1mLZwu5W"
   },
   "source": [
    "You should see that `ds_train_example` contains two items:\n",
    "\n",
    "1. The first is a tissue image. It has dimensions of `150, 150, 3`. This means that we have `150x150px` as the image size, with `3` color channels. The data type is `uint8`, which is an unsigned integer with 8 bits – so it can store values from `0..255`, which is exactly what we need for colors.\n",
    "2. The second is of type `int64` with just a single element. This contains the label.\n",
    "\n",
    "So, both the data and the label are stored in a single datastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QaOyozc0wu5W",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3df361fb283b8933bcefa9ccef3a0b5a",
     "grade": false,
     "grade_id": "cell-c2394694afb18127",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4: Draw the example image\n",
    "\n",
    "Even though we only loaded a single item with `take()`, it is still contained in a data structure that could potentially store more than one item. Therefore, we need to iterate over `ds_train_example` to access the items. The easiest way is a `for` loop. In Python, it can directly extract both contained data items of the tuple into separate variables.\n",
    "\n",
    "The first line of your code should then look like this: `for img, label in ds_train_example:`\n",
    "\n",
    "Inside the `for` loop, execute two things:\n",
    "\n",
    "1. **Print the *shape* of the `img` variable, as well as the `label`.** Use Python string formatting that we took a look at the beginning of the course to create a nicely readable output.  \n",
    "Note: if you directly print the `label` variable, you will see that it is a [Tensor](https://www.tensorflow.org/guide/tensor). This is a special data structure of TensorFlow, but very similar to a numpy array. To print the label as text, convert it to a numpy data type through `label.numpy()`.\n",
    "2. **Plot the image.** At the beginning, you imported `matplotlib.pylot` with an alias. It contains a convenient function called `imshow()`. Use this to plot the image `img` to the screen. This is just a single and very short line of code.\n",
    "\n",
    "Remember that in Python, everything that belongs to a `for` loop needs to have the same indentation level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "deletable": false,
    "id": "HR4wNBsDwu5X",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d4ac93bfa9f3a7b44348796734e7f17",
     "grade": false,
     "grade_id": "plot_image",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "b8709524-bdc9-48e6-e3e8-0cde782fa6df"
   },
   "outputs": [],
   "source": [
    "# Use a for loop to iterate over ds_train_example.\n",
    "# Inside the loop, print the shape and label of the current sample, and use plt.imshow to view the image.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bpXkSLpowu5X",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08446f8abd9339456e78da5bc2432eec",
     "grade": false,
     "grade_id": "cell-cc0bc0cc661f71d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following cell, assign the textual name of the image label to a new variable called `example_label`. Look up the label number with the list of label names we printed before (or even if you like the full name in the [research paper](https://www.nature.com/articles/srep27988))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "LA5qi69cwu5X",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23e49b06a5748d41e5f582ee831c37c1",
     "grade": false,
     "grade_id": "example_label_name",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a (string) variable called example_label where you assign the textuaL label name of the sample you printed above.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "o5BNGRBVwu5Y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "816e415738d467c3b7f1c1934bb5113e",
     "grade": true,
     "grade_id": "example_label_name_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert example_label != None\n",
    "import hashlib\n",
    "assert hashlib.blake2b(example_label.casefold().encode()).hexdigest() == '601d93b5ed6586571134fb5efb8e8c0ea8a4b49e9ab560a3315e21c0237e8843c9bc1ffe4153f61ad4212770276c9c0382d9ed1a0965b4eb7c2de31969189615'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avm0588rwu5Z"
   },
   "source": [
    "The method you created above always works for datasets and can print information about a contained item. The *Tensorflow Datasets* module we're using in this example actually has an even nicer method of showing the first few examples of a dataset.\n",
    "\n",
    "The corresponding function is `tfds.show_examples`. It needs two parameters: first the dataset itself (e.g,. `ds_train` or `ds_test`), and second the `ds_info` containing the label names.\n",
    "\n",
    "Run the function to see the 9 first examples of the `ds_test` dataset. Assign the returned object to a new variable called `fig` to avoid showing it two times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "deletable": false,
    "id": "bVSlb0Aswu5a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e8ffb742a202fce63e06c702887df1e",
     "grade": false,
     "grade_id": "plot_examples",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "c10b6c81-c1f9-4a19-b1d6-0ac6c2e9eff4"
   },
   "outputs": [],
   "source": [
    "# Use tfds.show_examples to print the first examples of the ds_test dataset\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6rp4ImDwu5a"
   },
   "source": [
    "One more example so that you can see that it's all connected :)\n",
    "\n",
    "You can also directly convert the data to a Pandas dataframe. Use `tfds.as_dataframe`. For the first parameter, specify some samples you want to visualize - e.g., use the `take()`  function as before to take the first 4 examples from `ds_train`. As the second parameter, specify `ds_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "deletable": false,
    "id": "LpnJ8wOUwu5b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b873784c2834b169ebdb1298fc81051c",
     "grade": false,
     "grade_id": "plot_dataframe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "483b5f17-b536-48ad-a5a2-5d751dbf2e0e"
   },
   "outputs": [],
   "source": [
    "# Use tfds.as_dataframe to convert some samples to a Pandas Dataframe and use this to visualize the examples.\n",
    "# Take the first 4 samples of ds_train, and also provide ds_info as the second parameter.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NLI7MZ8cwu5c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a7c3f2bdd15994aa9b35d0e1b621e48",
     "grade": false,
     "grade_id": "cell-a3fd37f28b523146",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5: Image Pre-Processing\n",
    "\n",
    "First of all, let's take a look at the raw data of an image. Simply print the `img` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "YRpwXE5rwu5c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d69e7472370e7b1ad087d85db613f0a",
     "grade": false,
     "grade_id": "raw_img",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "d270a3c1-6723-4cda-ea93-c4156fb6e543"
   },
   "outputs": [],
   "source": [
    "# Print the img variable\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR5rVX60wu5c"
   },
   "source": [
    "You can see in the output that `img` is a multi-dimensional array. The image contains the intensity values of all three colors (red, green, blue). These are scaled from 0..255. As we learned before, Neural Networks work best if you normalize the images. You should scale the values to a range 0..1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "F3_13F9qwu5d",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "382c8758d48421f0a01de03566a9932e",
     "grade": false,
     "grade_id": "cell-e78ae47439b0ae7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6: Input Pipeline\n",
    "\n",
    "Execute the following code block so that Python knows about our normalization function. It's taken from the [TensorFlow documentation](https://www.tensorflow.org/datasets/performances#caching_the_dataset). But essentially, it's the same as in our [MNIST example from the YouTube video](https://youtu.be/yNlBNp8KORA?t=777); the operation simply isn't performed on the data in memory (which is impossible for huge datasets), but instead added to the input pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Wt6ldsgkwu5d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25cb19a6a05e046de7c795be48db0724",
     "grade": false,
     "grade_id": "normalize_img_function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined code: simply execute this cell\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk-_thC3wu5d"
   },
   "source": [
    "We just defined a function which Tensorflow can apply to the images from our dataset. Next, we need to define the pipeline of how the actual raw images get to the machine learning model.\n",
    "\n",
    "The first step is to apply the normalization function to the image data. This is done through the `map` function of the dataset. Finally, simply store the updated pipeline in the original variable. The line should therefore look like this: `ds_train = ds_train.map(normalize_img)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "NqkOUtYgwu5d",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2715263d7e1f57549dc0a054ef6dc370",
     "grade": false,
     "grade_id": "normalize_img",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Apply the normalization function to the image data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxv8VewTwu5e"
   },
   "source": [
    "As the next step in our input pipeline, let's shuffle the data. We will go through the dataset multiple times (called *epochs*). It's a good idea to have the order of items randomized each time – especially, as we don't know if the classes are occuring randomly in the dataset. Otherwise, it could happen that a particular batch of the training data only contains examples of a single class, which makes it difficult for the model to learn how to distinguish classes.\n",
    "\n",
    "In the same way as before, add a `shuffle(4000)` step to our `ds_train` pipeline. `4000` is just the number of data items in our training set (80% of the 5000), so that the whole dataset is shuffled. Remember to re-assign the returned value to `ds_train` again, like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "vT5I-oQswu5e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e5f7eb017698085b151f48ea8008b76",
     "grade": false,
     "grade_id": "shuffle",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add shuffle with a parameter 4000 to ds_train and assign it back to ds_train \n",
    "# (like in the previous step)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCpyS5N-wu5e"
   },
   "source": [
    "As we have a large amount of data, it's a good idea to use *batching* with mini-batches as introduced in the theoretical part. This means that the model can update the neural network weights after each batch, and doesn't need to run through the entire dataset before it can update the weights. This makes learning faster and usually helps a bit to prevent over-fitting. As we have 8 different classes, a bigger batch size of for example 128 is good, so that multiple classes are present in each batch.\n",
    "\n",
    "Like before, add `batch(128)` to our input pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "xPkxLylrwu5e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f319ec43b7887ba31d0f6e7ec18e235",
     "grade": false,
     "grade_id": "batch",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add batching to the training dataset, with a batch size of 128\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKfYor90wu5e"
   },
   "source": [
    "The last two steps are optional, but help a bit to make machine learing faster. They make sure that TensorFlow optimally uses the different cores of your CPU and GPU so that for example one processor core of your PC can prepare the next batch, while another core is busy training based on another batch. Simply execute the next code block to add these steps to our pre-processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oezDmK6vwu5f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9708cd9b8566fac3849c2d9bd93c7886",
     "grade": false,
     "grade_id": "pipeline-optional",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined code: simply execute this cell\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUvT_0zQwu5f"
   },
   "source": [
    "Great – our training data pipeline is ready! Now, we need to perform the same steps on the `ds_test` data. Use the same code as before in the following cell, but make sure you always replace `ds_train` with `ds_test`. The only difference: do not use the `shuffle()` step. Shuffling the test data is not needed to evaluate the performance of our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "1w66S44Nwu5f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a0e89f1b7eaae35546ae2a18905cb12",
     "grade": false,
     "grade_id": "test_pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a pipeline for the ds_test dataset. Include the following steps:\n",
    "# 1. map using the normalize_img function\n",
    "# 2. batch(128)\n",
    "# 3. cache\n",
    "# 4. prefetch\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vVRRCSYywu5g",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d97218be0450e17447d552db7f0bf0cd",
     "grade": true,
     "grade_id": "pipeline_test",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "# If the tests fail, restart and run all cells. You might have executed\n",
    "# code blocks that add steps to the input pipeline multiple times, which\n",
    "# can have unintended side-effects.\n",
    "ds_example = ds_test.take(1)\n",
    "for (img,x) in ds_example:\n",
    "    assert img.shape == (128,150,150,3)\n",
    "    assert (img[0][0][0][0] >= 0)\n",
    "    assert (img[0][0][0][0] <= 1)\n",
    "ds_example = ds_train.take(1)\n",
    "for (img,x) in ds_example:\n",
    "    assert img.shape == (128,150,150,3)\n",
    "    assert (img[0][0][0][0] >= 0)\n",
    "    assert (img[0][0][0][0] <= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "R-ijGV12wu5h",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8aa238d79ebd080433ec1cfa8b93cf3a",
     "grade": false,
     "grade_id": "cell-2138287c5b7660e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 7: Build the Convolutional Neural Network\n",
    "\n",
    "Now, we're finally at the step where we define the structure of the neural network! The following would be a good architecture to start with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6xVxQP2wu5h"
   },
   "source": [
    "![Sample architecture of the CNN](https://github.com/andijakl/MachineLearning/raw/main/lab%203%20-%20deep%20learning%20-%20colorectal%20cancer/lab3b-architecture.png)*Sample architecture of the CNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mS2XGy7Pwu5h"
   },
   "source": [
    "Usually, a CNN has multiple convolutional layers, each followed by a max-pooling layer. At the end, after a flattening layer, two dense (fully connected) layers ensure that the outputs of the convolutional blocks are classified. The last layer is using the *softmax* activation function with 8 neurons, so that the probability for each of the 8 classes can be predicted.\n",
    "\n",
    "**Your task:** build an architecture like the one in the image above. You can use the architecture implementation of Lab 3a as reference.\n",
    "\n",
    "**Some hints:**\n",
    "\n",
    "* Typical definition of a convolutional layer:  \n",
    "`tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))`  \n",
    "Indiviual parameters:  \n",
    "  1. Define the **number of filters**. You could for example start with 32, and then go up to 64 layers for the following layers. Refer to the image above. The number of layers is printed above the image, e.g., `32@148x148` means that we have 3 layers and the images have a shape of `148x148` (missing 1 border pixel on each side due to the `3x3` convolution).\n",
    "  2. **Size of the filters** in this layer, for example `3x3`. Note that the filter size is not visible in the image above; the size is indicating the image size at this stage - it starts at `150x150`, gets a bit smaller with each convolutional layer due to the border pixels and is halved with each max-pool layer.\n",
    "  3. **Activation function** to use. Use the `relu` function.\n",
    "  4. The **input shape**: only necessary for the first layer. In this case, it's the image size that we already took a look at before – 150x150px with 3 colors.\n",
    "* Typical definition of a Max-Pool layer:  \n",
    "`tf.keras.layers.MaxPooling2D((2, 2))`  \n",
    "There is only a single parameter – the size of the pool. Always use a `2x2` pool. This already halves both the width and height of the data, resulting in a 1/4 of the original size. As we start with rather small 150x150px images, you shouldn't go too low too quickly.\n",
    "* The flattening is simple and doesn't have parameters we need to set:  \n",
    "`tf.keras.layers.Flatten()`\n",
    "* The last two layers are dense layers, like:  \n",
    "`tf.keras.layers.Dense(8, activation='softmax')`  \n",
    "Use 128 nodes for the first dense layer and the `relu` activation function. The second dense layer should have 8 nodes (according to the target classes) with the `softmax` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Qp9T92-gwu5i",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25498b44261770f32cf90779000007a8",
     "grade": false,
     "grade_id": "sequential-cnn-model",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a variable \"model\" and use the Sequential() function to add a list of layers according to the\n",
    "# description and the image above.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JbNtOaYMwu5i",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e68f2a183e170b4ca7d5ce534c252ea1",
     "grade": true,
     "grade_id": "model_test",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert type(model) == tf.keras.models.Sequential\n",
    "assert type(model.layers[0]) == tf.keras.layers.Conv2D\n",
    "assert type(model.layers[1]) == tf.keras.layers.MaxPooling2D \n",
    "assert type(model.layers[2]) == tf.keras.layers.Conv2D\n",
    "assert type(model.layers[3]) == tf.keras.layers.MaxPooling2D \n",
    "assert type(model.layers[4]) == tf.keras.layers.Conv2D\n",
    "assert type(model.layers[5]) == tf.keras.layers.MaxPooling2D \n",
    "assert type(model.layers[6]) == tf.keras.layers.Flatten \n",
    "assert type(model.layers[7]) == tf.keras.layers.Dense \n",
    "assert type(model.layers[8]) == tf.keras.layers.Dense \n",
    "assert model.layers[0].output_shape == (None, 148, 148, 32)\n",
    "assert model.layers[1].output_shape == (None, 74, 74, 32)\n",
    "assert model.layers[2].output_shape == (None, 72, 72, 64)\n",
    "assert model.layers[3].output_shape == (None, 36, 36, 64)\n",
    "assert model.layers[4].output_shape == (None, 34, 34, 64)\n",
    "assert model.layers[5].output_shape == (None, 17, 17, 64)\n",
    "assert model.layers[6].output_shape == (None, 18496)\n",
    "assert model.layers[7].output_shape == (None, 128)\n",
    "assert model.layers[8].output_shape == (None, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ycFpZvaMwu5i",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97dc5567504e4820c48c990fb4676f38",
     "grade": false,
     "grade_id": "cell-9624a1936a1513c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 8: Compile the Model\n",
    "\n",
    "After the model is defined, you also need to compile it. Use:\n",
    "\n",
    "* **Loss function:** `tf.keras.losses.SparceCategoricalCrossentropy()`. You do *not* need to set `from_logits=True` like in the previous example. Our last layer is `softmax`, so already results in a probability-like distribution that can be directly used by the categorical cross entropy loss.\n",
    "* **Optimizer:** use `tf.keras.optimizers.Adam()`, like in the previous example.\n",
    "* **Metrics:** use `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "DyOT-n-9wu5i",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3951c40c00eb293af687136f128049ed",
     "grade": false,
     "grade_id": "model_compile",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model according to the settings described above\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e8izKQmXwu5i",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4176072b43e0cdb834116a3c904a0e50",
     "grade": true,
     "grade_id": "model_compile_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert type(model.optimizer) == tf.keras.optimizers.Adam\n",
    "assert type(model.loss) == tf.keras.losses.SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0rA8zmQwu5j"
   },
   "source": [
    "Next, print the model summary. Note the total number of *trainable parameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "qyV7Eqofwu5j",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dd81fe5848228c9b978b7d1d3c41da8",
     "grade": false,
     "grade_id": "model_summary",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "c4ef6904-cf62-4b7a-f581-abb57aac6c4b"
   },
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzCYjVkyM5bi"
   },
   "source": [
    "How many parameters will need to be trained by our neural network? Create a variable `train_parameters` and store the number as an integer (do not use thousand separators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "M0s7Q3PWM4Xq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bed40080a594ff004d1cc811c9cc0de1",
     "grade": false,
     "grade_id": "train-parameters",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a variable train_parameters and insert the value of trainable params \n",
    "# from the model summary above\n",
    "train_parameters = 0\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oHK1ixrlNQUC",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7319572f9f00a5efc4269f220bee71e",
     "grade": true,
     "grade_id": "train-parameters-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute.\n",
    "assert train_parameters > 0\n",
    "assert isinstance(train_parameters, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WB0effywu5j"
   },
   "source": [
    "For reference, if you built the model exactly as shown in the architecture image, the summary should look like this:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/andijakl/MachineLearning/raw/main/lab%203%20-%20deep%20learning%20-%20colorectal%20cancer/lab3b-model-summary.png\" alt=\"Example model summary\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Z0X6DFFFwu5j",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "895ec162dc47b18ca81a7421a65c8943",
     "grade": false,
     "grade_id": "cell-f189eca0380543b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 9: Training\n",
    "\n",
    "Prepare a cup of coffee / tea for the next step. Depending on your computer speed, this might take several minutes. After all, we're working with real data and are training a deep neural network.\n",
    "\n",
    "Use the `fit` function of the model to train it based on `ds_train`. Also use the parameter `epochs` and train for at least 2 epochs, depending on your computer speed. This specifies how many iterations the training should make through the whole dataset (note that weight updates are performed already after each batch of 128 examples).\n",
    "\n",
    "With the CNN architecture from above, you can still get better results if you train for more epochs – but of course, it'll take a longer time. So let's start low; you can always increase the number of epochs once you know that you're on the right path and everything works.\n",
    "\n",
    "Save the returned training history in a variable called `history`, so that we can then visualize how loss and accuracy improved with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "cu6nK-WGwu5k",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa668553f7f5429cde23a7f14d0304dc",
     "grade": false,
     "grade_id": "model_fit",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "078f8796-77a9-476e-f8fc-877ce7bc7777"
   },
   "outputs": [],
   "source": [
    "# Fit the model based on the training data for 10 epochs.\n",
    "# Save the returned training info in a variable called history.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1cn3buBGwu5k",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "662115fe195f94f1c25fa2ab85958641",
     "grade": true,
     "grade_id": "model_fit_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert history.history['loss'][0] > 0\n",
    "assert history.history['accuracy'][0] > 0\n",
    "assert history.history['loss'][1] < history.history['loss'][0]\n",
    "assert history.history['accuracy'][1] > history.history['accuracy'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lREG-u72wu5k",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8efa1bf0afad3557d7adca9b2dd993c",
     "grade": false,
     "grade_id": "cell-056f8eca825bea00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 10: Visualize the History\n",
    "\n",
    "Now that your model is trained, let's visualize accuracy and loss. Like in the example from the lecture, Pandas provides an easy way to visualize the training history. Look up the code from [the YouTube video](https://youtu.be/yNlBNp8KORA?t=2462) and use it to draw the diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "deletable": false,
    "id": "U_HhFDZewu5k",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b25e1865b4dd64305ac1c8947c39414b",
     "grade": true,
     "grade_id": "visualize_history",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "2d9b0fa2-7ad8-4074-ef21-6537d5799f85"
   },
   "outputs": [],
   "source": [
    "# Convert history.history into a pandas DataFrame and use its plot function\n",
    "# to visualize how the accuracy increased during training and the loss was decreased.\n",
    "# If focusing on the accuracy (which can only be between 0..1), it makes sense to\n",
    "# limit the y axis to a range from (0,1).\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "T14PiG36wu5k",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6cfccc229e3c639fe50e75594a670e3",
     "grade": false,
     "grade_id": "cell-dd3ec78cc5b9f8cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 11: Evaluate the Model\n",
    "\n",
    "As you know, the accuracy on the training data is useful. But even more important is the performance of your neural network on new, unseen data.\n",
    "\n",
    "For this reason, we split the 5,000 examples into `ds_train` and `ds_test` at the beginning. Use the model's `evaluate()` function to compute the accuracy it achieves on the `ds_test` dataset! Most likely, it'll be lower than on the training data. But it should still be a really good result. Remember that we're distinguishing between 8 different target classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "kCZ6c5Xdwu5l",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e02fafb5688fece62b836c681cc1b6fb",
     "grade": false,
     "grade_id": "eval_model",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "aecd3068-85db-4701-aad6-fc39d579ad60"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model with the test set.\n",
    "# Save the results in a variable eval_results. It will contain the loss and accuracy values\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Mqlbdemzwu5l",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af77c4467a4e88fc54d11a908ed28ef2",
     "grade": true,
     "grade_id": "eval_model_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert eval_results[0] > 0\n",
    "assert eval_results[1] > 0.5\n",
    "assert eval_results[1] < 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "B_57etXUwu5l",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca6ca60328fa79e4e3b2cebf6dd8f64c",
     "grade": false,
     "grade_id": "cell-ff3dc51d701222ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 12: Predict Examples\n",
    "\n",
    "Let's see how the model performed on some examples of the test set.\n",
    "\n",
    "First, let's take the first piece of our `ds_test` dataset. We're using the same method as before with `take(1)`. Store the returned data in a new variable called `ds_predict_example`.\n",
    "\n",
    "As `ds_test` is now accessed through the TensorFlow input pipeline, we don't just get 1 example, but instead 1 batch – consisting of 128 examples (our batch size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "lY1_huNGwu5l",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92ffc60208eb2046702373e32f43492b",
     "grade": false,
     "grade_id": "predict_example",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Take one batch from ds_test and store it in a variable called ds_predict_example\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bvNAVkogwu5l",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ca25cc20a443c9f7dc5cf9961eb84f0",
     "grade": true,
     "grade_id": "predict_example_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert list(ds_predict_example.as_numpy_iterator())[0][1].size == 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LWjwuO4wu5m"
   },
   "source": [
    "Now, let's use our trained model to predict the probabilities from our dataset slice. Call the `predict()` function of our model. Supply the new variable `ds_predict_example` that we just created as argument. Store the outputs in a new variable called `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "XJ2l6zXZwu5m",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "146efa1234704c1733cc3db04ba42435",
     "grade": false,
     "grade_id": "predict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Predict probabilities for all 128 examples.\n",
    "# Store prediction results in variable predictions\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tgEozwMcwu5m",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eaa02b85393a6170685b073e85a3be14",
     "grade": true,
     "grade_id": "predict_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert predictions.shape == (128,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjIpoCNqwu5m"
   },
   "source": [
    "Let's take a look at the raw probabilities. For visualizing tabular data, Pandas is always a good choice, as it provides a nicer formatting. Create a Pandas dataframe called `df_predictions`. Give it our `predictions` variable as input.\n",
    "\n",
    "To see the class labels as column labels, assign these to the `columns` argument when creating the dataframe. As we already printed at the beginning of this notebook, you can access an array of label names through `ds_info.features[\"label\"].names`.\n",
    "\n",
    "Note: You don't see an output of this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "BqQvqc-Rwu5m",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "550396c9c21158cbed40646c121c1b10",
     "grade": false,
     "grade_id": "predict_df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe based on the predictions variable.\n",
    "# Call the dataframe df_predictions\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Yr1gPirowu5n",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02ff3389ed1b0d92b247938bdd243ea8",
     "grade": true,
     "grade_id": "predict_df_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert type(df_predictions) == pd.DataFrame\n",
    "assert df_predictions.size == 1024\n",
    "assert df_predictions.shape == (128, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCqp42Ymwu5n"
   },
   "source": [
    "Next, let's show the dataframe – simply write the `df_predictions` variable name into the next cell. You'll see that it outputs the full numbers, which are not very readable. Therefore, modify the line and call the function `.round(decimals=2)` on the dataframe.\n",
    "\n",
    "The columns correspond to the classes with their respective labels. The rows to the data items (`0..127`, the batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "deletable": false,
    "id": "3whGjHXrwu5n",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af176e741509c8796afb442ccdd9ec42",
     "grade": false,
     "grade_id": "predict_print",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "9a453020-557d-49ae-c922-07b32b803157"
   },
   "outputs": [],
   "source": [
    "# Display uses the nice way of drawing Pandas Dataframes, even if there are multiple outputs in this cell\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM08CJgCwu5o"
   },
   "source": [
    "You'll see that the neural network usually wasn't 100% sure what is the best class. Sometimes, the maximum in each row is more distinct, in other cases the network has a harder time to decide between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AxBzw_xewu5o",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5e84df8860ee3999a2c5340a9c8f4e2",
     "grade": false,
     "grade_id": "cell-8b7240f695157536",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 13: Visualize Predictions\n",
    "\n",
    "Let's take a look at individual data items, print their true label, the predicted label and the image itself.\n",
    "\n",
    "In the code block below, the variables have already been defined. Append your code to fill in these variables. The following code block then visualizes the variables, with short descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4KGMvrpdtC0"
   },
   "source": [
    "**Your Task:** \n",
    "The variable called `show_item` selects which item in our batch to show. Set it to a number between `0` and `127`. This is the item from the predicted batch that we want to visualize.\n",
    "\n",
    "Next, create the **loop** -> `for img, label in ds_predict_example:` (as before) to get data out of our dataset. As said, the dataset is organized in batches, consisting of 128 examples. Our `for`-loop doesn't return an individual item, but the whole batch. (Even though we only retrieved a single batch. Thus, the loop is only executed once). However, we'll just visualize a single item from the batch, with the index that we've stored in `show_item`. \n",
    "\n",
    "**Inside the loop**, assign the info of the single item into the four pre-defined variables:\n",
    "\n",
    "1. **The true label (use variable: `true_label` for the final result):** the `label` variable from the for loop contains all the 128 labels of the batch. Therefore, simply access the item we need from the array: `label[show_item]`. As before, this returns a tensor. Make it easily printable by converting it to a numpy item with `.numpy()`. We also want to get the label instead of the numberic class. You can get this through `ds_info.features[\"label\"].names[<numeric class label>]`\n",
    "2. **The predicted label (variable: `predicted_label`):** our neural network always predicts the class that has the max probability. Our predictions are stored in the Pandas dataframe `df_predictions`, which we just created in the previous step. There's an easy way to get what we want – just apply both statements on the dataframe and store the final result in `predicted_label`:\n",
    "  1. **Accessing rows:** to access a row of a Pandas dataframe by its index, use `df_predictions.iloc[show_item]`. This returns all class probabilities for a single item.\n",
    "  2. **Label of the maximum value:** the `.max()` function gives you the maximum value (= probability) in this row. While useful, here we would actually need the class label (= column index) with the max probability instead. The `.idxmax()` function does just that, so use it on the row you just retrieved.\n",
    "3. **All probabilities (variable: `predicted_label_probs`):** To get an impression of how sure the network was about its prediction, also retrieve all probabilities. In the same way as before, access a single row of `df_predictions` using `iloc`. Then, `round()` the output to *two* decimals.\n",
    "4. **Show the image (variable: `img_data`):** To print the image later, we'll convert the raw data of the image to a numpy array. To do this, call `.numpy()` on `img[show_item]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "feprmpvewu5p",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "671874b65098c8c4451b13fb25411eab",
     "grade": false,
     "grade_id": "show_prediction",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify the variables created here. They will be assigend to later.\n",
    "true_label = None\n",
    "predicted_label = None\n",
    "predicted_label_probs = None\n",
    "img_data = None\n",
    "\n",
    "# Modify the following line to choose which item to visualize.\n",
    "# You can assign any number to show_item from 0..127\n",
    "show_item = None\n",
    "\n",
    "# Start coding below, according to the instructions from the block above to fill the variables.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vve1R5rawu5p"
   },
   "source": [
    "After coding the loop above, execute the following block of code to see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "deletable": false,
    "editable": false,
    "id": "bWfxJzhCwu5p",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d37f8711edc03142b28158b53dfda276",
     "grade": false,
     "grade_id": "show_prediction_print",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f81e25af-6f42-41c8-a925-b0636c35975e"
   },
   "outputs": [],
   "source": [
    "# No need to modify this code, just execute it after writing your code\n",
    "# in the previous code block. It should visualize the results of the prediction of a single item.\n",
    "print(\"Showing item:\", show_item)\n",
    "print(\"True label:\", true_label)\n",
    "print(\"Predicted label with max probability:\", predicted_label)\n",
    "print(\"Predicted label probabilities:\")\n",
    "print(predicted_label_probs)\n",
    "plt.imshow(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsHNoCqYwu5q"
   },
   "source": [
    "For reference, this is an example of what your output could look like. Note that the probabilities are most likely different, depending on how long you trained the model. Also the item could be a different one.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/andijakl/MachineLearning/raw/main/lab%203%20-%20deep%20learning%20-%20colorectal%20cancer/lab3b-prediction.png\" alt=\"Example prediction\" width=\"250\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HnJ4FFJPwu5q",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8fcf0db1a056b78a5a6aa1d26f75cfd",
     "grade": true,
     "grade_id": "show_prediction_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pre-defined tests to check your code. Do not modify, just execute this cell.\n",
    "assert show_item >= 0 and show_item < 128\n",
    "assert true_label is not None\n",
    "assert predicted_label is not None\n",
    "assert predicted_label_probs.size == 8\n",
    "assert np.isclose(np.sum(predicted_label_probs), 1.0, rtol=0.1), \"Should be close to 0.99 / 1.00\"\n",
    "assert img_data.shape == (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzGS8MBrwu5r"
   },
   "source": [
    "## Next Steps (Optional)\n",
    "\n",
    "When you've reached this point, it's time to hand in your exercise!\n",
    "\n",
    "But of course, now the fun part could start - trying to figure out ways how to improve the model to achieve better results. You could add additional convolutional & max-pool layers to your model. Or of course, just train for more epochs – even though you might run into overfitting if you train too long, which would then be visible if your training accuracy is far above the test accuracy. You could tweak the number or size of the filters.\n",
    "\n",
    "But that's outside of the exercise. Make a private copy of the notebook if you wish to modify the network structure or parameters!\n",
    "\n",
    "*(Note: with this approach, you're optimizing the results for the `ds_test` set, to make its accuracy as high as possible during evaluation. Thus, you're probably overfitting on the test set. To circumvent that, a usual approach is to have training data for learning the model, validation data to improve the model parameters, and use the test set only to get a good real-life estimate of the final model. We didn't go this route in this example, so that we have more examples for the training & testing sets. The main goal of this lab is to get the neural network to work. But if you enjoyed this work so far, there's a whole world of great ways of how you can fine-tune the model, and strategies for preventing overfitting (e.g., dropout) that you can look into. But these are all details – in this lecture, you've come a long way and can be really proud of what you achieved!)*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JzGS8MBrwu5r"
   ],
   "name": "ex3-part2-deep-learning-22ss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
