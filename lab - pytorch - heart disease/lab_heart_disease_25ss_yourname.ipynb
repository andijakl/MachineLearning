{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i66GHKbc40_R",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1d8c5548342de9fa",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Exercise: Heart Disease / PyTorch\n",
        "\n",
        "Version: 4.0, Summer Semester 2025\n",
        "\n",
        "Follow the steps to load the heart disease database and to train & score a Neural Network classifier with it.\n",
        "The database is a slightly modified version from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/heart+Disease)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZZPUzLg40_b",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d3d3fde835de04ed",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Your Name\n",
        "\n",
        "Enter your name in the block below:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enter your name here {\"run\":\"auto\",\"vertical-output\":true}\n",
        "student_name = '' # @param {type:\"string\"}\n",
        "import uuid\n",
        "import hashlib\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "notebook_version = 4.0\n",
        "\n",
        "def getData():\n",
        "    mid = hashlib.sha256(str(uuid.getnode()).encode()).hexdigest()[:10]\n",
        "    execution_time = datetime.now().isoformat()\n",
        "    return mid, execution_time\n",
        "\n",
        "mid, execution_time = getData()\n",
        "\n",
        "# Store metadata in the notebook itself\n",
        "print(f\"Your name: {student_name} ({notebook_version} - {mid} - {execution_time})\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_4C2dlfkM3xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The blocks with assert perform automated tests so that you know if your solution is correct, without giving away how to code it. Simply execute these blocks. If you don't get any output from them, you know that your code is correct."
      ],
      "metadata": {
        "id": "l1hiFQ40PsSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert student_name != '', \"Please enter your name in the block above\""
      ],
      "metadata": {
        "id": "Q1E9o_mNPqEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H68AKRSV40_j",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-62706836af48625e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 1: Read about the Dataset\n",
        "\n",
        "Check out the dataset description of [UCI](https://archive.ics.uci.edu/ml/datasets/heart+Disease). Our dataset is the Cleveland set and uses the smaller variant with 14 attributes. Take a look at column 3 (\"chest pain\" / \"cp\"). The values are either 1, 2, 3 or 4 in the dataset.\n",
        "\n",
        "Change the code below so that the Python dictionary contains the names of the four different types of chest pain that the dataset distinguishes.\n",
        "\n",
        "*Hint: you'll find the answer in the \"Attribute information\" section of the linked web page.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2jp7ibu40_k",
        "nbgrader": {
          "grade": false,
          "grade_id": "chest-pain",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Replace xxx with the textual names of the different types of chest pain\n",
        "pain_types = {1: \"xxx\",\n",
        "             2: \"xxx\",\n",
        "             3: \"xxx\",\n",
        "             4: \"xxx\"}\n",
        "# When you have finished replacing the values, delete the exception in the line\n",
        "# below and run the next block with the asserts to see if you got it right.\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWv8Xe9g40_l",
        "nbgrader": {
          "grade": true,
          "grade_id": "chest-pain-test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "assert hashlib.blake2b(pain_types[1].casefold().encode()).hexdigest() == '10de5546a056feb6522445a2e94492fa20419030d70694bd97fc67016906936dd8a8791e286becb0bb5f8087986bd2621f2f06d639523f653f955a2d45e78d6f'\n",
        "assert hashlib.blake2b(pain_types[2].casefold().encode()).hexdigest() == '884a2e513fec27229c985dc4b01d4935d42e90e6c71d1f5d519b38e7369ae23be525d803dadf9e102ddb91beb6feef4a4d71d8a9636c1a4106f87c1aee1c2640'\n",
        "assert hashlib.blake2b(pain_types[3].casefold().encode()).hexdigest() == 'cb8436129905c921a1930a01e2ae2ebe74d5562826802d6ea27f183c6703a3a58d9e8cc94f5eb53e48928abcefdd88e8ac50e153c99f0870005e327fcd4d1c89'\n",
        "assert hashlib.blake2b(pain_types[4].casefold().encode()).hexdigest() == '56f3734df8320938c6c2e66fec6ea97f04e6f5f7e5cc27c57da98fc99ba277442240d7c0a9d4aa12816199274de33843c591d6d61037cd5a12f347f50850bd5b'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxycN9pu40_n",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4547db3a672318c2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 2: Imports\n",
        "\n",
        "Use the following block for all the imports you need for your notebook.\n",
        "This will include `pandas` (as `pd`), `numpy` (as `np`), `seaborn` (as `sns`) and all the parts of `sklearn` you will need. Also include all necessary imports for PyTorch (PyTorch itself, `nn`, `optim`, `TensorDataset` and `DataLoader`).\n",
        "Configure `matplotlib` to draw graphics inline to ensure the figures are visible inside the notebook. If during this notebook you discover that you need additional imports, just come back to this cell, add the import here and execute the cell again. It's often easier to have all imports in one place.\n",
        "\n",
        "In the rest of the notebook, simply replace `raise NotImplementedError` with your own code and then execute the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT0TuDpz40_4",
        "nbgrader": {
          "grade": false,
          "grade_id": "imports",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Place all neccessary imports here\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJgiMqeQ45Rp"
      },
      "outputs": [],
      "source": [
        "# Predefined imports - just execute this line\n",
        "# This is needed for some of the automated tests to help you check if your\n",
        "# code is correct\n",
        "import unittest\n",
        "test_case = unittest.TestCase()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmPMXqK140_6",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-e4aff452f88702b6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 3: Load the CSV\n",
        "\n",
        "Load the `heart_disease-fhstp-nomissing.csv` file into a Pandas dataframe variable called `df_heart` and print its head to check if importing worked. The URL for the file is: `https://raw.githubusercontent.com/andijakl/MachineLearning/refs/heads/main/lab%20-%20pytorch%20-%20heart%20disease/heart_disease-fhstp-nomissing.csv`\n",
        "\n",
        "You can read CSV files with `read_csv` from the pandas library you imported as `pd`. The dataset we are using has been modified from the original to remove lines where data was missing. Therefore, it has 297 lines instead of the original 303."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LksKi_uV40_7",
        "nbgrader": {
          "grade": false,
          "grade_id": "read-csv",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "file_url = 'https://raw.githubusercontent.com/andijakl/MachineLearning/refs/heads/main/lab%20-%20pytorch%20-%20heart%20disease/heart_disease-fhstp-nomissing.csv'\n",
        "# Load the dataset into a variable called df_heart\n",
        "# Specify that ? should be recognized as na value\n",
        "# Print the head of the dataframe to see if importing worked correctly.\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER9Gmfnw40_9",
        "nbgrader": {
          "grade": true,
          "grade_id": "read-csv-test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Tests if data has been loaded\n",
        "assert len(df_heart) == 297, \"Imported data should have 297 samples\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88NZ_TUw41AH",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3f3b82b5b65e36ee",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 4: Describe\n",
        "\n",
        "Use the `describe` function of the Pandas DataFrame to find out the count, mean, standard deviation & more from the dataset for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbZurMAs41AI",
        "nbgrader": {
          "grade": false,
          "grade_id": "describe",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Call method to describe the dataset\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sylhUSQI41AJ"
      },
      "source": [
        "Based on the printed information you see about the dataset, answer a few questions and assign the numbers to the corresponding variables. Round your answers (up/down) to the next integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tnTDY1m41AK",
        "nbgrader": {
          "grade": false,
          "grade_id": "dataset_data",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# What is the mean age?\n",
        "mean_age = 0;\n",
        "# What is the maximum heart rate?\n",
        "max_hr = 0;\n",
        "# What is the standard deviation of the cholesterol level?\n",
        "std_cholesterol = 0;\n",
        "# Simply assign the values to the variables above and when finished,\n",
        "# remove the line throwing the not implemented error.\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPwyLZlv41AL",
        "nbgrader": {
          "grade": true,
          "grade_id": "dataset_data_tests",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert mean_age > 0\n",
        "assert max_hr > 0\n",
        "assert std_cholesterol > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E48myLhD41AL",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0d9766f98ece4159",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5: Age Histogram\n",
        "\n",
        "Print the histogram of the \"age\" column. Use `10` bins. This gives you a good understanding of the patients that were tested for heart diseases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCYDiuB741AM",
        "nbgrader": {
          "grade": false,
          "grade_id": "histogram",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Call method to print the age histogram with 10 bins\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0rmsRHB41AM"
      },
      "source": [
        "According to the age bins above (middle number), which age is most frequent in the dataset? Look at the histogram and assign what you think should be approximately in the middle of the highest bin to the following variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2bgFQva41AN",
        "nbgrader": {
          "grade": false,
          "grade_id": "histogram-question",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Change the variable to the approximate middle of the highest bin in the histogram above.\n",
        "# This answers: from what age are most people in the dataset? Assign that age to most_age.\n",
        "most_age = -1\n",
        "# Again, assign your answer to the variable above and then delete the exception\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jKJ4jNM41AN",
        "nbgrader": {
          "grade": true,
          "grade_id": "histogram-test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert most_age != -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IySIkEhb41AO",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8b1ce0df5e624a4e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 6: Violin Plot\n",
        "\n",
        "Use the Violin Plot of Seaborn (add the import statement at the beginning of the file!) to print the target class (`\"diameter narrowing\"` -> *diagnosis of heart disease (angiographic disease status), Value 0: < 50% diameter narrowing*) on the `x` axis and `\"max HR\"` on the `y` axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Eg7_5941AO",
        "nbgrader": {
          "grade": false,
          "grade_id": "violin-plot",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Plot a violin plot with the parameters stated above\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aJr5wx641AP"
      },
      "source": [
        "Based on the violin plot, answer: is the max HR higher if the target class is 0 or 1?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRuPAL0n41AP",
        "nbgrader": {
          "grade": false,
          "grade_id": "max-hr-question",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Change the variable max_hr_higher to\n",
        "# 0 if the maximum heart rate is higher if there is no heart disease;\n",
        "# 1 if the maximum heart rate is higher if there is heart disease\n",
        "max_hr_higher = -1\n",
        "# Assign the value to the variable above and delete the exception\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_sfHADE41AW",
        "nbgrader": {
          "grade": true,
          "grade_id": "max-hr-test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert max_hr_higher != -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJsQe81M41AX",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-05082cf9060880b1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 7: Prepare Data for Classification\n",
        "\n",
        "Our CSV contains all the data plus the target class values. Remove (`pop`) the target class into an extra variable (`y`).\n",
        "Then split the dataframe into the 4 variables for training & test data. Use the random state `10`. You don't need to specify the size of the training and testing set; according to the [train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), the default size of the test data will be 0.25, the training data 0.75."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzVXwnmO41AX",
        "nbgrader": {
          "grade": false,
          "grade_id": "target-class",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Pop the target class from df_heart, convert it to a numpy array using to_numpy()\n",
        "# And store the resulting target array in a variable y\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REZtuVG741AY",
        "nbgrader": {
          "grade": true,
          "grade_id": "target-class-test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert y.shape == (297,)\n",
        "assert y[1] == 1\n",
        "assert df_heart.shape == (297,13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We1Y7-BP41AY",
        "nbgrader": {
          "grade": false,
          "grade_id": "split",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Split the dataframe and the target classes using the random state 10\n",
        "# into the variables X_train, X_test, y_train and y_test\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15uvalXg41AZ",
        "nbgrader": {
          "grade": true,
          "grade_id": "split-test",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert X_train.shape == (222,13)\n",
        "assert y_train.shape == (222,)\n",
        "assert X_test.iloc[0]['age'] == 60.0\n",
        "assert X_train.iloc[5]['age'] == 46.0\n",
        "assert X_test.shape == (75, 13)\n",
        "assert y_test.shape == (75,)\n",
        "assert y_test[1] == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odcBSE1R41Ap",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f99e8cf42d7f1661",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 8: Scaling\n",
        "\n",
        "Many classifiers like Neural Networks work best with scaled features. Therefore, we need to scale these to make the features more comparable.\n",
        "\n",
        "Create the `StandardScaler`. Then use `fit_transform()` to analyze the training data and to scale it in one step. Afterwards, use `transform()` to scale the test data (this applies the same scaling that was learned from the training data).\n",
        "Make sure to save the scaled test + train data into new variables, e.g., `X_train_scaled` and `X_test_scaled`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmVcPBIn41Aq",
        "nbgrader": {
          "grade": false,
          "grade_id": "scaler-create",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Create the standard scaler and store it in a variable called scaler\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVIeObP-41Ar",
        "nbgrader": {
          "grade": false,
          "grade_id": "scaler-transform",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Correctly scale the train and test data into X_train_scaled and X_test_scaled\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-QkUpIH41Ar",
        "nbgrader": {
          "grade": true,
          "grade_id": "scaler-test",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "assert X_train_scaled.shape == (222,13)\n",
        "assert X_test_scaled.shape == (75,13)\n",
        "assert np.isclose(X_train_scaled[0][0], 1.0153, rtol=0.1), \"Should be close to 1.1053\"\n",
        "assert np.isclose(X_test_scaled[0][0], 0.5599, rtol=0.1), \"Should be close to 0.5599\"\n",
        "assert np.isclose(X_test_scaled[1][1], 0.7143, rtol=0.1), \"Should be close to 0.7143\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3yaUGII5NLV"
      },
      "source": [
        "## 9: PyTorch Setup\n",
        "\n",
        "In this block, we will convert the data to tensors and into data loaders so that they can be used by PyTorch.\n",
        "\n",
        "First, convert the four data variables (`X_train_scaled` etc.) to tensors, using similar names but indicating that these have the tensor form (`X_train_tensor` etc.).\n",
        "\n",
        "Also specify the `dtype` as `torch.float32`, to ensure classification using the standard neural network settings will work afterwards.\n",
        "\n",
        "For the `y` target classes, you also need to add the following at the end of the conversion: `.view(-1,1)`. What does this do?\n",
        "\n",
        "- Without the conversion, the tensors would have the shape of (number_of_samples,), meaning 1-dimensional vectors\n",
        "- With the conversion, the tensors are reshaped to (number_of_samples, 1). This makes them 2-dimensional tensors with a single column.\n",
        "\n",
        "Why is this needed? Most neural network architectures and standard functions expect the target tensor to have two dimensions. Also, the model output will be in the same shape; to compare the predictions, both arrays need to have the same shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srACj3Tx5MhI"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors. Make sure you use the scaled values!\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWaCZ1RS5CRo"
      },
      "outputs": [],
      "source": [
        "test_case.assertEqual(X_train_tensor.shape, (222, 13))\n",
        "test_case.assertEqual(X_test_tensor.shape, (75, 13))\n",
        "test_case.assertEqual(y_train_tensor.shape, (222, 1))\n",
        "test_case.assertEqual(y_test_tensor.shape, (75, 1))\n",
        "\n",
        "test_case.assertEqual(X_train_tensor.dtype, torch.float32)\n",
        "test_case.assertEqual(X_test_tensor.dtype, torch.float32)\n",
        "test_case.assertEqual(y_train_tensor.dtype, torch.float32)\n",
        "test_case.assertEqual(y_test_tensor.dtype, torch.float32)\n",
        "\n",
        "test_case.assertTrue(torch.isclose(X_train_tensor[0][0], torch.tensor(1.0153, dtype=torch.float32), rtol=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqw1cGfl7vSo"
      },
      "source": [
        "Next, create two `TensorDataset` variables (`train_dataset` and `test_dataset`) that combine the `X` and `y` for each dataset. This makes it easier for PyTorch to handle the data.\n",
        "\n",
        "Afterwards, construct a `DataLoader` for each dataset. Name these variables: `train_loader` and `test_loader`. They will take care of loading the next batch into memory for training. As such, set a `batch_size` of `32`. It's good if you let the `DataLoader` shuffle the training dataset between each epoch, to avoid any effects the order of data might have on the training. For consistency during evaluation, set `shuffle` to `False` for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5h2Fw7S5h_G"
      },
      "outputs": [],
      "source": [
        "# Create TensorDataset and DataLoader\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCocgUA_5XLT"
      },
      "outputs": [],
      "source": [
        "test_case.assertEqual(len(train_dataset), 222)\n",
        "test_case.assertEqual(len(test_dataset), 75)\n",
        "\n",
        "# Check the first batch to ensure data loaders are working as intended.\n",
        "for x, y in train_loader:\n",
        "    test_case.assertEqual(x.shape[1], 13)  # Check the number of features\n",
        "    test_case.assertEqual(y.shape[1], 1)  # Check the number of target variables\n",
        "    break\n",
        "\n",
        "for x, y in test_loader:\n",
        "    test_case.assertEqual(x.shape[1], 13)  # Check the number of features\n",
        "    test_case.assertEqual(y.shape[1], 1)  # Check the number of target variables\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUbvoxcB5t1b"
      },
      "source": [
        "## 10: Define Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOg5oLym6SZh"
      },
      "source": [
        "Now, define the structure of the neural network using `nn.Sequential`. The resulting variable that stores the network should be called `model`.\n",
        "\n",
        "Layers:\n",
        "\n",
        "1. **Linear:** this fully connected layer needs two attributes: the features of our input data (which is 13, according to the 13 different health indicators). To make the code more dynamic, you can also use `X_train_scaled.shape[1]` to retrieve the number. The second attribute specifies the number of nodes (neurons) in the first hidden layer. Set it to 16.\n",
        "2. **ReLU:** Add a ReLU (Rectified Linear Unit) activation function. This is crucial for learning complex patterns in data, as it introduces non-linearity into the network.\n",
        "3. **Linear:** the second hidden layer. Take the 16 outputs from before as inputs, and provide 8 outputs for the following layer.\n",
        "4. **ReLU:** another ReLU activation function is applied after the second hidden layer.\n",
        "5. **Linear** output layer: it takes 8 outputs from the previous layer as inputs and produces a single output value. This will represent the model's prediction.\n",
        "6. **Sigmoid:** it squashes the output value between 0 and 1, representing the probability of the input belonging to one of the two classes (heart disease or no heart disease).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJYY5DUz5j0R"
      },
      "outputs": [],
      "source": [
        "# Define the neural network model using Sequential\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80NQPU5S_Ukb"
      },
      "outputs": [],
      "source": [
        "test_case.assertEqual(len(list(model.children())), 6)  # Check for 6 layers\n",
        "test_case.assertIsInstance(model[0], nn.Linear) # Check first layer is Linear\n",
        "test_case.assertIsInstance(model[1], nn.ReLU) # Check second layer is ReLU\n",
        "test_case.assertIsInstance(model[2], nn.Linear) # Check third layer is Linear\n",
        "test_case.assertIsInstance(model[3], nn.ReLU) # Check fourth layer is ReLU\n",
        "test_case.assertIsInstance(model[4], nn.Linear) # Check fifth layer is Linear\n",
        "test_case.assertIsInstance(model[5], nn.Sigmoid) # Check sixth layer is Sigmoid\n",
        "\n",
        "test_case.assertEqual(model[0].in_features, 13) #check input size of first layer\n",
        "test_case.assertEqual(model[0].out_features, 16) #check output size of first layer\n",
        "test_case.assertEqual(model[2].in_features, 16) #check input size of second layer\n",
        "test_case.assertEqual(model[2].out_features, 8) #check output size of second layer\n",
        "test_case.assertEqual(model[4].in_features, 8) #check input size of third layer\n",
        "test_case.assertEqual(model[4].out_features, 1) #check output size of third layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPTKbY_QAL1x"
      },
      "source": [
        "Now print the summary of your created model. Import summary from torchsummary (pre-installed on Google CoLab).\n",
        "\n",
        "Then, use its summary function and send it the model. The second parameter is a tuple that specifies the input size that the model expects.\n",
        "- The first parameter is the batch size. You defined this above when creating the `DataLoader`s.\n",
        "- The second is the number of features. You can dynamically get the number like you did previously when defining the input shape of the first neural network layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwys6tid6Crx"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS_IRknQeeEk"
      },
      "source": [
        "The last preparation steps are the loss function and the optimizer:\n",
        "\n",
        "* For the loss, use `BCELoss`, which is the binary cross entropy for classification. We are performing a binary classification.\n",
        "* For the optimizer, use `Adam`. You can define the learning rate yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A68V54i6LbH"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-nwfqg64E1"
      },
      "source": [
        "## 11: Train the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yZNyKwpeyP6"
      },
      "source": [
        "First, let's define two variables.\n",
        "\n",
        "* The first is `num_epochs`. Store the number of training epochs you would like to use, e.g., 50. It's good practice to have this in a variable to make changes in configuration easier in a central place, without directly messing around in code of the for-loop.\n",
        "\n",
        "* Also define a variable `train_loss_history` as an empty array. We'll append values during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytuF6qr466nh"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WmhVvj9ML3A"
      },
      "outputs": [],
      "source": [
        "assert num_epochs > 0\n",
        "assert len(train_loss_history) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgMZyETI_7oP"
      },
      "source": [
        "Set the model to **train mode**. We don't evaluate within the loop, so it's enough to only switch to train mode once before the loop. This is important because some layers, like dropout, behave differently during training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4UtFfts_9Mj"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sif6yuP5MDvI"
      },
      "outputs": [],
      "source": [
        "assert model.training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uIE0slOff2x"
      },
      "source": [
        "Now it's time for the large training loop. Use the standard procedure for PyTorch:\n",
        "\n",
        "\n",
        "1. Create a `for`-loop over the range of `num_epochs`.\n",
        "2. Set a `total_loss` variable to `0`, so that the loss of each epoch can be summed while going through batches.\n",
        "3. Create an inner `for`-loop over the `train_loader`. It returns two variables that you have as loop variables. Call them: `batch_X`, `batch_y`.\n",
        "  1. Within the inner loop, first **reset the optimizer** using `zero_grad()`.\n",
        "  2. Next, send your batch data to the **model** and store its **predictions** in a `y_pred` variable.\n",
        "  3. Use the predicted and the true labels to calculate the **loss function** you defined above. Store its results in a variable called `loss`.\n",
        "  4. Call `backward()` on `loss` to compute the gradients of the loss function with respect to all trainable parameters.\n",
        "  5. **Update the parameters** of the model using `optimizer.step()` using our Adam optimization function you defined before.\n",
        "  **6. For statistics:** add the `loss.item()` to the total_loss variable so that we can get the sum of all losses of the whole epoch. *Note:* this calculates the average loss per batch, which is sufficient as we want to monitor the trend and not the exact loss per item.\n",
        "\n",
        "4. Outside of the inner loop: **Compute the average loss**. Divide the `total_loss` by the number of items we used for training (--> `len(train_loader)`).\n",
        "5. Append the average loss you computed to the `train_loss_history` array.\n",
        "\n",
        "6. Every 5 epochs, **print** the current epoch number as well as the current training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkXqmaaP69FB"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn5obOQLMRtT"
      },
      "outputs": [],
      "source": [
        "# Assert statements to check training loop correctness\n",
        "assert len(train_loss_history) == num_epochs, \"Training loss history should have an entry for each epoch.\"\n",
        "\n",
        "# Check for decreasing loss trend (not strictly decreasing, allowing for minor fluctuations)\n",
        "for i in range(1, len(train_loss_history) - 5):  # Check for a trend over several epochs\n",
        "    assert train_loss_history[i] <= train_loss_history[i-1] * 1.5, f\"Loss should be non-increasing. Epoch {i}: {train_loss_history[i]}, Epoch {i-1}: {train_loss_history[i - 1]}\"\n",
        "\n",
        "# Check final loss value\n",
        "assert train_loss_history[-1] < 0.5, f\"Final training loss is unexpectedly high: {train_loss_history[-1]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QaL1lAa31PT"
      },
      "source": [
        "## 12: Plot the training history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHMlgr8PieZS"
      },
      "source": [
        "Execute the following block of code to see the visualization of the training loss. It should decrease during training over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I7G2uVxIS1C"
      },
      "outputs": [],
      "source": [
        "# Plot Training Loss and Test Accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.plot(train_loss_history, label=\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "mid, execution_time = getData()\n",
        "plt.text(.01, .01, f'{notebook_version}, {mid}, {execution_time}', ha='left', va='bottom', transform=plt.gca().transAxes)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-5XG85vikeg"
      },
      "source": [
        "## 13: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsifaoik9eOx"
      },
      "source": [
        "As the last step, we also want to know how well our model performs. We'll need to evaluate the trained model with the separate test data, which was not used for training. Therefore, the model has never seen it before, making the data a more reliable indicator on the model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY41R4IVAIBa"
      },
      "source": [
        "First, we need to set up a few things. Create two variables, `correct` and `total`, and set both to 0. We'll use these for counting how many predictions we got correct, compared to the ground truth.\n",
        "\n",
        "Also, set the `model` to *evaluation mode*. Among other things, this disables dropout features if defined in the structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1gF66-cAFzW"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUI6weaAMnWq"
      },
      "outputs": [],
      "source": [
        "assert model.training == False\n",
        "assert correct == 0\n",
        "assert total == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXqDIwpxA3rN"
      },
      "source": [
        "The evaluation is again a loop. This time, we iterate over the `test_loader`. We count the number of correctly classified items.\n",
        "\n",
        "First, we use the `no_grad` context manager of PyTorch, which disables gradient calculation in its block. This reduces memory consumption for computations. When the code execution leaves the block, it will automatically re-enable gradient calculation.\n",
        "\n",
        "Within this block, we iterate over the `test_loader` with a `for`-loop, similar to the training loop. As loop variables, we get both the data for the batch (`batch_X`), as well as the labels (`batch_y`).\n",
        "\n",
        "Within the loop:\n",
        "1. Let the `model` **predict** based on the batch data, and store the results in `y_pred`.\n",
        "2. **Convert the probabilities to binary labels.** How? The last layer of our neural network is a sigmoid. This means that it returns float values between [0..1]. If we don't weight one class more, we take the middle (`0.5`) as separation point. Therefore, we can just compare if `y_pred >= 0.5` to get the class output (`0` or `1`). To be on the safe side, convert the output with float() to prevent binary labels (true / false), as our y is just containing 0/1, but also formatted as float. Store this in `y_pred_class`.\n",
        "3. **Count how many items are classified correctly.** To do this, compare if `y_pred_class == batch_y`. This would return a vector of 0 or 1, depending on whether the classes are equal (`1`) or not (`0`). To calculate the accuracy, we need to know how many are correct, so use `sum()` to count how many correct predictions we have in that array. Finally, we're now leaving the tensor-world and returning to plain Python numbers, so convert the tensor to a number with `item()`. Add this result to the `correct` variable.\n",
        "4. The final step is to **count how many items** we have processed in total so far, which will later be required for the percentage. Add `batch_y.size(0)` to the `total` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaahIWF2fP8q"
      },
      "outputs": [],
      "source": [
        "# Evaluate accuracy on test set\n",
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmFAoHbJMxBE"
      },
      "outputs": [],
      "source": [
        "# Assertions for the evaluation part\n",
        "assert total == 75, \"Total number of test samples should be 75.\"\n",
        "accuracy = correct / total\n",
        "assert accuracy >= 0.7, \"Accuracy should be above 70%.\" # Adjust threshold as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBgASIiBKJdq"
      },
      "source": [
        "Now we have all the numbers, and we just need to calculate the accuracy: divide correct by total and print the result for our test accuracy. With the default architecture, you should reach around 90% arruracy. The exact number will differ every time you run the example, as the solution the neural network found depends on the random initial value of the weights and biases initally assigned when constructing the network. Another big influence would be the split between the training and test set, but we made sure this is consistent by supplying the inital value for the random number generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTcsjlHUA7E1"
      },
      "outputs": [],
      "source": [
        "raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6a0chC7L_GK"
      },
      "outputs": [],
      "source": [
        "assert test_accuracy > 0.80\n",
        "assert test_accuracy < 0.95\n",
        "execution_time = datetime.now().isoformat()\n",
        "print(f\"Execution time: {execution_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hpyLmX141Ax",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f3b904833da5d331",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 14: Final Checks\n",
        "\n",
        "To make sure that your whole Jupyter Notebook executes without issues, choose *Runtime -> Restart Session and Run All ...*. Make sure it actually restarts executing all cells â€“ if not, select the command again. Make sure all the lines you wrote execute and all automated tests pass."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jupyter automatically saves the cell outputs into the notebook itself, so make sure the cell outputs (including the plotted graphs) are visible. To submit your notebook with all the outputs included, follow these steps:\n",
        "1. Rename the file to include your name, e.g.: \"lab_heart_disease_25ss_**jakl**.ipynb\"\n",
        "2. Make sure your file is saved (press Ctrl+S)\n",
        "3. Export the notebook to your computer: File --> Download --> Download .ipynb\n",
        "4. Go to eCampus and upload your notebook file to the assignment."
      ],
      "metadata": {
        "id": "NP3GgcLDQk1o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppp1iKDHTvj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}